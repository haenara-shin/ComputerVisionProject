{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"opencv_yolov3_inference.ipynb","provenance":[{"file_id":"1BOcOGcQ_eXq6ZuGG2fGWbt97jc16a3GJ","timestamp":1625208617755}],"collapsed_sections":[]},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZVRgTwfuY4TV"},"source":["### OpenCV Darknet Yolo를 이용하여 이미지Object Detection\n","* yolo와 tiny-yolo 를 이용하여 Object Detection"]},{"cell_type":"markdown","metadata":{"id":"WiAh0hGQ_LWC"},"source":["#### 입력 이미지로 사용될 이미지 다운로드/보기"]},{"cell_type":"code","metadata":{"id":"8mqbLhc-Y4TY"},"source":["!mkdir /content/data\n","!wget -O ./data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4UhN_QQjY4Tm"},"source":["#### Darknet Yolo사이트에서 coco로 학습된 Inference모델와 환경파일을 다운로드 받은 후 이를 이용해 OpenCV에서 Inference 모델 생성\n","\n","* https://pjreddie.com/darknet/yolo/ 에 다운로드 URL 있음.\n","* pretrained 모델은 wget https://pjreddie.com/media/files/yolov3.weights 에서 다운로드\n","* pretrained 모델을 위한 환경 파일은 https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg 에서 다운로드\n","* wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true -O ./yolov3.cfg\n","* readNetFromDarknet(config파일, weight파일)로 config파일 인자가 weight파일 인자보다 먼저 옴. 주의 필요. \n","\n","* tiny yolo의 pretrained된 weight파일은 wget https://pjreddie.com/media/files/yolov3-tiny.weights 에서 download 가능. \n","* config 파일은 wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg?raw=true -O ./yolov3-tiny.cfg 로 다운로드"]},{"cell_type":"code","metadata":{"id":"xJEpALOjaqhI"},"source":["### coco 데이터 세트로 pretrained 된 yolo weight 파일과 config 파일 다운로드하여 /content/pretrained 디렉토리 아래에 저장. \n","!mkdir ./pretrained\n","!echo \"##### downloading pretrained yolo/tiny-yolo weight file and config file\"\n","!wget -O /content/pretrained/yolov3.weights https://pjreddie.com/media/files/yolov3.weights\n","!wget -O /content/pretrained/yolov3.cfg https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true \n","\n","!wget -O /content/pretrained/yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights\n","!wget -O /content/pretrained/yolov3-tiny.cfg https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg?raw=true\n","\n","!ls /content/pretrained\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwh5Qtx0Y4Tr"},"source":["#### readNetFromDarknet(config파일, weight파일)을 이용하여 yolo inference network 모델을 로딩"]},{"cell_type":"code","metadata":{"id":"7SyFlTzWY4Tu"},"source":["import os\n","import cv2\n","\n","weights_path = '/content/pretrained/yolov3.weights'\n","config_path =  '/content/pretrained/yolov3.cfg'\n","#config 파일 인자가 먼저 옴. \n","cv_net_yolo = cv2.dnn.readNetFromDarknet(config_path, weights_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYJysoTaY4T1"},"source":["#### COCO class id와 class 명 매핑"]},{"cell_type":"code","metadata":{"id":"P8DLZ_A2Y4T2"},"source":["labels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n","                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n","                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n","                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n","                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n","                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n","                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n","                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x2p5o3ppY4UE"},"source":["#### 3개의 scale Output layer에서 결과 데이터 추출"]},{"cell_type":"code","metadata":{"id":"38bbQIdYAxA6"},"source":["layer_names = cv_net_yolo.getLayerNames()\n","print('### yolo v3 layer name:', layer_names)\n","print('final output layer id:', cv_net_yolo.getUnconnectedOutLayers())\n","print('final output layer name:', [layer_names[i[0] - 1] for i in cv_net_yolo.getUnconnectedOutLayers()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8a87BcFY4UF"},"source":["#전체 Darknet layer에서 13x13 grid, 26x26, 52x52 grid에서 detect된 Output layer만 filtering\n","layer_names = cv_net_yolo.getLayerNames()\n","outlayer_names = [layer_names[i[0] - 1] for i in cv_net_yolo.getUnconnectedOutLayers()]\n","print('output_layer name:', outlayer_names)\n","\n","img = cv2.imread('./data/beatles01.jpg')\n","img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# 로딩한 모델은 Yolov3 416 x 416 모델임. 원본 이미지 배열을 사이즈 (416, 416)으로, BGR을 RGB로 변환하여 배열 입력\n","cv_net_yolo.setInput(cv2.dnn.blobFromImage(img, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False))\n","\n","# Object Detection 수행하여 결과를 cvOut으로 반환 \n","cv_outs = cv_net_yolo.forward(outlayer_names)\n","print('cv_outs type:', type(cv_outs), 'cv_outs의 내부 원소개수:', len(cv_outs))\n","print(cv_outs[0].shape, cv_outs[1].shape, cv_outs[2].shape)\n","print(cv_outs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RPNgIBSoY4UN"},"source":["#### 3개의 scale output layer에서 Object Detection 정보를 모두 수집. \n","* center와 width,height좌표는 모두 좌상단, 우하단 좌표로 변경. "]},{"cell_type":"code","metadata":{"id":"Ny1mZ8JsY4UP"},"source":["import numpy as np\n","\n","# 원본 이미지를 네트웍에 입력시에는 (416, 416)로 resize 함. \n","# 이후 결과가 출력되면 resize된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\n","rows = img.shape[0]\n","cols = img.shape[1]\n","\n","conf_threshold = 0.5\n","nms_threshold = 0.4\n","\n","# bounding box의 테두리와 caption 글자색 지정\n","green_color=(0, 255, 0)\n","red_color=(0, 0, 255)\n","\n","class_ids = []\n","confidences = []\n","boxes = []\n","\n","# 3개의 개별 output layer별로 Detect된 Object들에 대해서 Detection 정보 추출 및 시각화 \n","for ix, output in enumerate(cv_outs):\n","    print('output shape:', output.shape)\n","    # feature map에 있는 anchor 갯수만큼 iteration하면서 Detected 된 Object 추출.(13x13x3, 26x26x3, 52x52x3)\n","    for jx, detection in enumerate(output):\n","        # class score는 detetection배열에서 5번째 이후 위치에 있는 값. \n","        class_scores = detection[5:]\n","        # class_scores배열에서 가장 높은 값을 가지는 값이 class confidence, 그리고 그때의 위치 인덱스가 class id\n","        class_id = np.argmax(class_scores)\n","        confidence = class_scores[class_id]\n","\n","        # confidence가 지정된 conf_threshold보다 작은 값은 제외 \n","        if confidence > conf_threshold:\n","            print('ix:', ix, 'jx:', jx, 'class_id', class_id, 'confidence:', confidence)\n","            # detection은 scale된 좌상단, 우하단 좌표를 반환하는 것이 아니라, detection object의 중심좌표와 너비/높이를 반환\n","            # 원본 이미지에 맞게 scale 적용 및 좌상단, 우하단 좌표 계산\n","            center_x = int(detection[0] * cols)\n","            center_y = int(detection[1] * rows)\n","            width = int(detection[2] * cols)\n","            height = int(detection[3] * rows)\n","            left = int(center_x - width / 2)\n","            top = int(center_y - height / 2)\n","            # 3개의 개별 output layer별로 Detect된 Object들에 대한 class id, confidence, 좌표정보를 모두 수집\n","            class_ids.append(class_id)\n","            confidences.append(float(confidence))\n","            boxes.append([left, top, width, height])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLp5YthuY4UU"},"source":["#### NMS를 이용하여 각 Output layer에서 Detected된 Object의 겹치는 Bounding box를 제외. "]},{"cell_type":"code","metadata":{"id":"TGH3xLopY4UV"},"source":["conf_threshold = 0.5\n","nms_threshold = 0.4\n","idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n","\n","idxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GyZwFco-KlYO"},"source":["idxs.flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WuDutomPY4Ua"},"source":["#### NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화."]},{"cell_type":"code","metadata":{"id":"lsHrDsEkY4Ub"},"source":["import matplotlib.pyplot as plt\n","\n","# cv2의 rectangle()은 인자로 들어온 이미지 배열에 직접 사각형을 업데이트 하므로 그림 표현을 위한 별도의 이미지 배열 생성. \n","draw_img = img.copy()\n","\n","# NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화.\n","if len(idxs) > 0:\n","    for i in idxs.flatten():\n","        box = boxes[i]\n","        left = box[0]\n","        top = box[1]\n","        width = box[2]\n","        height = box[3]\n","        # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.\n","        caption = \"{}: {:.4f}\".format(labels_to_names_seq[class_ids[i]], confidences[i])\n","        #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n","        cv2.rectangle(draw_img, (int(left), int(top)), (int(left+width), int(top+height)), color=green_color, thickness=2)\n","        cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, red_color, 1)\n","        print(caption)\n","\n","img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n","plt.figure(figsize=(12, 12))\n","plt.imshow(img_rgb)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsFeIbSOY4Uh"},"source":["#### 단일 이미지를 Yolo로 detect하는 get_detected_img() 함수 생성. "]},{"cell_type":"code","metadata":{"id":"_RH-zcYKY4Ui"},"source":["def get_detected_img(cv_net, img_array, conf_threshold, nms_threshold, is_print=True):\n","    \n","    # 원본 이미지를 네트웍에 입력시에는 (416, 416)로 resize 함. \n","    # 이후 결과가 출력되면 resize된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\n","    rows = img_array.shape[0]\n","    cols = img_array.shape[1]\n","    \n","    draw_img = img_array.copy()\n","    \n","    #전체 Darknet layer에서 13x13 grid, 26x26, 52x52 grid에서 detect된 Output layer만 filtering\n","    layer_names = cv_net.getLayerNames()\n","    outlayer_names = [layer_names[i[0] - 1] for i in cv_net.getUnconnectedOutLayers()]\n","    \n","    # 로딩한 모델은 Yolov3 416 x 416 모델임. 원본 이미지 배열을 사이즈 (416, 416)으로, BGR을 RGB로 변환하여 배열 입력\n","    cv_net.setInput(cv2.dnn.blobFromImage(img_array, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False))\n","    start = time.time()\n","    # Object Detection 수행하여 결과를 cvOut으로 반환 \n","    cv_outs = cv_net.forward(outlayer_names)\n","    layerOutputs = cv_net.forward(outlayer_names)\n","    # bounding box의 테두리와 caption 글자색 지정\n","    green_color=(0, 255, 0)\n","    red_color=(0, 0, 255)\n","\n","    class_ids = []\n","    confidences = []\n","    boxes = []\n","\n","    # 3개의 개별 output layer별로 Detect된 Object들에 대해서 Detection 정보 추출 및 시각화 \n","    for ix, output in enumerate(cv_outs):\n","        # Detected된 Object별 iteration\n","        for jx, detection in enumerate(output):\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            # confidence가 지정된 conf_threshold보다 작은 값은 제외 \n","            if confidence > conf_threshold:\n","                #print('ix:', ix, 'jx:', jx, 'class_id', class_id, 'confidence:', confidence)\n","                # detection은 scale된 좌상단, 우하단 좌표를 반환하는 것이 아니라, detection object의 중심좌표와 너비/높이를 반환\n","                # 원본 이미지에 맞게 scale 적용 및 좌상단, 우하단 좌표 계산\n","                center_x = int(detection[0] * cols)\n","                center_y = int(detection[1] * rows)\n","                width = int(detection[2] * cols)\n","                height = int(detection[3] * rows)\n","                left = int(center_x - width / 2)\n","                top = int(center_y - height / 2)\n","                # 3개의 개별 output layer별로 Detect된 Object들에 대한 class id, confidence, 좌표정보를 모두 수집\n","                class_ids.append(class_id)\n","                confidences.append(float(confidence))\n","                boxes.append([left, top, width, height])\n","    \n","    # NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화.\n","    idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n","    if len(idxs) > 0:\n","        for i in idxs.flatten():\n","            box = boxes[i]\n","            left = box[0]\n","            top = box[1]\n","            width = box[2]\n","            height = box[3]\n","            # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.\n","            caption = \"{}: {:.4f}\".format(labels_to_names_seq[class_ids[i]], confidences[i])\n","            #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n","            cv2.rectangle(draw_img, (int(left), int(top)), (int(left+width), int(top+height)), color=green_color, thickness=2)\n","            cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, red_color, 1)\n","\n","    if is_print:\n","        print('Detection 수행시간:',round(time.time() - start, 2),\"초\")\n","    return draw_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3zyudBZY4Uo"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import time\n","import os\n","\n","# image 로드 \n","img = cv2.imread('/content/data/beatles01.jpg')\n","\n","weights_path = '/content/pretrained/yolov3.weights'\n","config_path =  '/content/pretrained/yolov3.cfg'\n","\n","# darknet yolo pretrained 모델 로딩\n","cv_net_yolo = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMJQefNJGJRR"},"source":["conf_threshold = 0.5\n","nms_threshold = 0.4\n","# Object Detetion 수행 후 시각화 \n","draw_img = get_detected_img(cv_net_yolo, img, conf_threshold=conf_threshold, nms_threshold=nms_threshold, is_print=True)\n","\n","img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(img_rgb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqIoa11rY4VB"},"source":["### tiny Yolo로 Object Detection 수행하기. "]},{"cell_type":"code","metadata":{"id":"VC5ngVUfY4VC"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import time\n","import os\n","\n","# image 로드 \n","img = cv2.imread('/content/data/beatles01.jpg')\n","\n","weights_path = '/content/pretrained/yolov3-tiny.weights'\n","config_path =  '/content/pretrained/yolov3-tiny.cfg'\n","\n","# darknet tiny yolo pretrained 모델 로딩\n","cv_net_yolo_tiny = cv2.dnn.readNetFromDarknet(config_path, weights_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0odKbPoGf_N"},"source":["conf_threshold = 0.2\n","nms_threshold = 0.4\n","# Object Detetion 수행 후 시각화 \n","draw_img = get_detected_img(cv_net_yolo_tiny, img, conf_threshold=conf_threshold, nms_threshold=nms_threshold, is_print=True)\n","\n","img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(img_rgb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1WL6DSMN0Kq"},"source":[""],"execution_count":null,"outputs":[]}]}
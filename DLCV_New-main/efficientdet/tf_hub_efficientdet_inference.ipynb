{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf_hub_efficientdet_inference.ipynb","provenance":[{"file_id":"14qPTo6CJZu6MWNkYWILOif7loNHKQcf1","timestamp":1624420928962}],"collapsed_sections":[],"authorship_tag":"ABX9TyPYHWOac14nkSl0RIn+xg++"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7EVO2EfndpGI"},"source":["import tensorflow as tf\n","#tensorflow_hub import 수행. \n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8iKt4kMdwCK"},"source":["print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrTNQgPjS2qH"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXN4__WEJIw3"},"source":["### 입력 이미지로 사용될 이미지 다운로드"]},{"cell_type":"code","metadata":{"id":"hh4wzlLRd1C2"},"source":["!mkdir /content/data\n","!wget -O ./data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5kTvu_jJRrK"},"source":["### TF Hub에서 EfficientDet d0 Inference 모델 다운로드 후 Inference 수행.\n","* 원하는 모델명은 TF Hub에서 검색해서 hub.lod()로 다운로드 후 tensorflow로 사용 가능할 수 있도록 로딩됨\n","* EfficientDet Tensorflow Object Detection API로 구현된 모델로 Download\n","* 로딩된 모델은 바로 원본 이미지로 Object Detection이 가능. 입력 값으로 numpy array, tensor 모두 가능하며 uint8로 구성 필요."]},{"cell_type":"code","metadata":{"id":"V1VzwZQJeEBf"},"source":["module_handle = \"https://tfhub.dev/tensorflow/efficientdet/d0/1\"\n","detector_model = hub.load(module_handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C10_0C7CL1_h"},"source":["import cv2\n","import time\n","import numpy as np\n","\n","img_array_np = cv2.imread('/content/data/beatles01.jpg')\n","img_array = img_array_np[np.newaxis, ...]\n","print(img_array_np.shape, img_array.shape)\n","\n","start_time = time.time()\n","# image를 detector_model에 인자로 입력하여 inference 수행. \n","result = detector_model(img_array)\n","print('elapsed time:', time.time()-start_time)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbJ0qxYANCcL"},"source":["img_tensor = tf.convert_to_tensor(img_array_np, dtype=tf.uint8)[tf.newaxis, ...]\n","start_time = time.time()\n","# image를 detector_model에 인자로 입력하여 inference 수행. \n","result = detector_model(img_tensor)\n","print('elapsed time:', time.time()-start_time)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phd2i1NXmtw7"},"source":["# image를 numpy가 아니라 tensor로 로딩\n","def load_img(path):\n","  img = tf.io.read_file(path)\n","  #png 파일일 경우 decode_png()호출 \n","  img = tf.image.decode_jpeg(img, channels=3)\n","  print(img.shape, type(img))\n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2RIpjism0-g"},"source":["import time \n","\n","# image를 tensor형태로 로딩. \n","img = load_img('/content/data/beatles01.jpg')\n","# 3차원 image tensor를 4차원 tensor로 변환. \n","# Efficientdet d0의 경우에는 입력 image를 unit8로 적용 필요. \n","converted_img = tf.image.convert_image_dtype(img, tf.uint8)[tf.newaxis, ...]\n","\n","start_time = time.time()\n","# image를 detector_model에 인자로 입력하여 inference 수행. \n","result = detector_model(converted_img)\n","print('elapsed time:', time.time()-start_time)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVTO0YjVNzKv"},"source":["### inference 수행 반환값 확인 \n","* inference 결과는 dictionary 형태로 반환되며, 개별 key값은 Object Detection 모델에 따라 달라질 수 있음. 개별 value는 tensor로 되어 있음. \n","* inference 반환된 bbox 좌표는 이미지 사이즈 대비 스케일링 된 0~1 사이 값이며 **ymin, xmin, ymax, xmax 형태로 반환되므로 반드시 주의 필요**"]},{"cell_type":"code","metadata":{"id":"Sma_o68wtqfP"},"source":["# inference 결과 출력. dictionary 형태의 출력 결과. dict내부의 key는 model 별로 서로 다름. 출력하여 key값 확인 필요. \n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bv3UF9hUrvKQ"},"source":["print(result.keys())\n","# detect 결과는 100개를 기본으로 Detect 함(즉 Detect된 오브젝트는 무조건 100개. 그래서 tensor(array)는 100개 단위, num_detections는 100) \n","print(result['detection_boxes'].shape, result['detection_classes'].shape,  result['detection_scores'].shape, result['num_detections'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tw_1LweEtA87"},"source":["# detect된 object들은 detection score가 높은 순으로 array값을 순차적으로 채움. \n","print('#### detection_classes #####')\n","print(result['detection_classes'])\n","print('#### detection_scores #####')\n","print(result['detection_scores'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6umP0QqtUqB"},"source":["# bounding box 좌표는 ymin, xmin, ymax, xmax 순서로 반환됨. y가 먼저, x가 나중에 나오므로 반드시 주의해야 함. \n","# 좌표 값은 원본 이미지의 width, height로 0~1 사이값으로 정규화됨. \n","print('#### detection_boxes #####')\n","print(result['detection_boxes'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NFaZKylK8rt"},"source":["### inference 결과를 이미지로 시각화"]},{"cell_type":"code","metadata":{"id":"FuwPZ_MUIeH_"},"source":["# result내의 value들을 모두 numpy로 변환. \n","result = {key:value.numpy() for key,value in result.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UF8HjtUDuY5N"},"source":["# 1부터 91까지의 COCO Class id 매핑. \n","labels_to_names = {1:'person',2:'bicycle',3:'car',4:'motorcycle',5:'airplane',6:'bus',7:'train',8:'truck',9:'boat',10:'traffic light',\n","                    11:'fire hydrant',12:'street sign',13:'stop sign',14:'parking meter',15:'bench',16:'bird',17:'cat',18:'dog',19:'horse',20:'sheep',\n","                    21:'cow',22:'elephant',23:'bear',24:'zebra',25:'giraffe',26:'hat',27:'backpack',28:'umbrella',29:'shoe',30:'eye glasses',\n","                    31:'handbag',32:'tie',33:'suitcase',34:'frisbee',35:'skis',36:'snowboard',37:'sports ball',38:'kite',39:'baseball bat',40:'baseball glove',\n","                    41:'skateboard',42:'surfboard',43:'tennis racket',44:'bottle',45:'plate',46:'wine glass',47:'cup',48:'fork',49:'knife',50:'spoon',\n","                    51:'bowl',52:'banana',53:'apple',54:'sandwich',55:'orange',56:'broccoli',57:'carrot',58:'hot dog',59:'pizza',60:'donut',\n","                    61:'cake',62:'chair',63:'couch',64:'potted plant',65:'bed',66:'mirror',67:'dining table',68:'window',69:'desk',70:'toilet',\n","                    71:'door',72:'tv',73:'laptop',74:'mouse',75:'remote',76:'keyboard',77:'cell phone',78:'microwave',79:'oven',80:'toaster',\n","                    81:'sink',82:'refrigerator',83:'blender',84:'book',85:'clock',86:'vase',87:'scissors',88:'teddy bear',89:'hair drier',90:'toothbrush',\n","                    91:'hair brush'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9257odbdwwG0"},"source":["def get_detector(module_handle=\"https://tfhub.dev/tensorflow/efficientdet/d0/1\"):\n","  detector = hub.load(module_handle)\n","  return detector\n","\n","detector_model = get_detector()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUnqprc8xHi8"},"source":["import cv2\n","\n","img_array = cv2.cvtColor(cv2.imread('/content/data/beatles01.jpg'), cv2.COLOR_BGR2RGB)\n","\n","# scaling된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\n","height = img_array.shape[0]\n","width = img_array.shape[1]\n","# cv2의 rectangle()은 인자로 들어온 이미지 배열에 직접 사각형을 업데이트 하므로 그림 표현을 위한 별도의 이미지 배열 생성. \n","draw_img = img_array.copy()\n","\n","# bounding box의 테두리와 caption 글자색 지정\n","green_color=(0, 255, 0)\n","red_color=(0, 0, 255)\n","\n","# cv2로 만들어진 numpy image array를 tensor로 변환\n","img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n","#img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)[tf.newaxis, ...]\n","\n","# pretrained 모델을 다운로드 한 뒤 inference 수행. \n","result = detector_model(img_tensor)\n","# result 내부의 value를 numpy 로 변환. \n","result = {key:value.numpy() for key,value in result.items()}\n","\n","SCORE_THRESHOLD = 0.3\n","OBJECT_DEFAULT_COUNT = 100\n","\n","# detected 된 object들을 iteration 하면서 정보 추출. detect된 object의 갯수는 100개\n","for i in range(min(result['detection_scores'][0].shape[0], OBJECT_DEFAULT_COUNT)):\n","  # detection score를 iteration시 마다 높은 순으로 추출하고 SCORE_THRESHOLD보다 낮으면 loop 중단. \n","  score = result['detection_scores'][0, i]\n","  if score < SCORE_THRESHOLD:\n","    break\n","  # detected된 object들은 scale된 기준으로 예측되었으므로 다시 원본 이미지 비율로 계산\n","  box = result['detection_boxes'][0, i]\n","\n","  ''' **** 주의 ******\n","   box는 ymin, xmin, ymax, xmax 순서로 되어 있음. '''\n","  left = box[1] * width\n","  top = box[0] * height\n","  right = box[3] * width\n","  bottom = box[2] * height\n","\n","  # class id 추출하고 class 명으로 매핑\n","  class_id = result['detection_classes'][0, i]\n","  caption = \"{}: {:.4f}\".format(labels_to_names[class_id], score)\n","  print(caption)\n","  #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n","  cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), color=green_color, thickness=2)\n","  cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, red_color, 1)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnneW8GbuZBg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rlol0rP1uZFY"},"source":["import time\n","\n","def get_detected_img(model, img_array, score_threshold, object_show_count=100, is_print=True):   \n","  # scaling된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\n","  height = img_array.shape[0]\n","  width = img_array.shape[1]\n","  # cv2의 rectangle()은 인자로 들어온 이미지 배열에 직접 사각형을 업데이트 하므로 그림 표현을 위한 별도의 이미지 배열 생성. \n","  draw_img = img_array.copy()\n","\n","  # bounding box의 테두리와 caption 글자색 지정\n","  green_color=(0, 255, 0)\n","  red_color=(0, 0, 255)\n","\n","  # cv2로 만들어진 numpy image array를 tensor로 변환\n","  img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n","  #img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)[tf.newaxis, ...]\n","\n","  # efficientdet모델로 inference 수행. \n","  start_time = time.time()\n","  # inference 결과로 내부 원소가 Tensor이 Dict 반환 \n","  result = model(img_tensor)\n","  # result 내부의 value를 numpy 로 변환. \n","  result = {key:value.numpy() for key,value in result.items()}\n","\n","  # detected 된 object들을 iteration 하면서 정보 추출. detect된 object의 갯수는 100개\n","  for i in range(min(result['detection_scores'][0].shape[0], object_show_count)):\n","    # detection score를 iteration시 마다 높은 순으로 추출하고 SCORE_THRESHOLD보다 낮으면 loop 중단. \n","    score = result['detection_scores'][0, i]\n","    if score < score_threshold:\n","      break\n","    # detected된 object들은 scale된 기준으로 예측되었으므로 다시 원본 이미지 비율로 계산\n","    box = result['detection_boxes'][0, i]\n","\n","    ''' **** 주의 ******\n","    box는 ymin, xmin, ymax, xmax 순서로 되어 있음. '''\n","    left = box[1] * width\n","    top = box[0] * height\n","    right = box[3] * width\n","    bottom = box[2] * height\n","\n","    # class id 추출하고 class 명으로 매핑\n","    class_id = result['detection_classes'][0, i]\n","    caption = \"{}: {:.4f}\".format(labels_to_names[class_id], score)\n","    print(caption)\n","    #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n","    cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), color=green_color, thickness=2)\n","    cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, red_color, 1)\n","\n","  if is_print:\n","    print('Detection 수행시간:',round(time.time() - start_time, 2),\"초\")\n","\n","  return draw_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09gXAtLpuZJF"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/beatles01.jpg'), cv2.COLOR_BGR2RGB)\n","draw_img = get_detected_img(detector_model, img_array, score_threshold=0.4, object_show_count=100, is_print=True)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gv8W_vjYOVZu"},"source":["!wget -O ./data/baseball01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/baseball01.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThHjAvpZOVdF"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/baseball01.jpg'), cv2.COLOR_BGR2RGB)\n","draw_img = get_detected_img(detector_model, img_array, score_threshold=0.4, object_show_count=100, is_print=True)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gK5hVS8WuKoJ"},"source":["### EfficientDet D2 모델로 Inference 수행. "]},{"cell_type":"code","metadata":{"id":"2DPLvF71EA4a"},"source":["detector_model_d2 = get_detector('https://tfhub.dev/tensorflow/efficientdet/d2/1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lSReAjxEBET"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/baseball01.jpg'), cv2.COLOR_BGR2RGB)\n","draw_img = get_detected_img(detector_model_d2, img_array, score_threshold=0.4, object_show_count=100, is_print=True)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCawNIyZPYda"},"source":["### Video Inference 수행"]},{"cell_type":"code","metadata":{"id":"PIMTDCMBOVjH"},"source":["!wget -O ./data/Jonh_Wick_small.mp4 https://github.com/chulminkw/DLCV/blob/master/data/video/John_Wick_small.mp4?raw=true"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mK2OBBbOVmc"},"source":["def do_detected_video(model, input_path, output_path, score_threshold, is_print):\n","    \n","    cap = cv2.VideoCapture(input_path)\n","\n","    codec = cv2.VideoWriter_fourcc(*'XVID')\n","\n","    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size) \n","\n","    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    print('총 Frame 갯수:', frame_cnt)\n","\n","    green_color=(0, 255, 0)\n","    red_color=(0, 0, 255)\n","    while True:\n","        hasFrame, img_frame = cap.read()\n","        if not hasFrame:\n","            print('더 이상 처리할 frame이 없습니다.')\n","            break\n","        \n","        img_frame = get_detected_img(model, img_frame, score_threshold=score_threshold, object_show_count=100, is_print=is_print)\n","        \n","        vid_writer.write(img_frame)\n","    # end of while loop\n","\n","    vid_writer.release()\n","    cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zet5NjkZOVoZ"},"source":["do_detected_video(detector_model, '/content/data/Jonh_Wick_small.mp4', './data/John_Wick_small_02.mp4', 0.5, True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jt_j5adPnWz_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TfN8bC6hucEF"},"source":["### EfficientDet lite0 Pretrained 모델 Inference 수행\n","* EfficientDet Lite는 automl 패키지로 구현됨. \n","* 입력 이미지로 numpy array, tensor 모두 가능, type은 unit8 필요. \n","* inference 결과로 box정보, score정보, class 정보를 각각 Tensor로 반환. "]},{"cell_type":"code","metadata":{"id":"zIB-VBUIuhjr"},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import cv2\n","import numpy as np\n","\n","detector_automl_lite0 = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/lite0/detection/1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMLvmKvEv6Ni"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/baseball01.jpg'), cv2.COLOR_BGR2RGB)\n","#img_array_01 = img_array[np.newaxis, ...]\n","img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n","\n","start_time = time.time()\n","# image를 detector_model에 인자로 입력하여 inference 수행. \n","#boxes, scores, classes, num_detections = detector_automl_lite0(img_array_01)\n","boxes, scores, classes, num_detections = detector_automl_lite0(img_tensor)\n","\n","print('elapsed time:', time.time()-start_time)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmHx4cYBxWsT"},"source":["boxes.shape, scores.shape, classes.shape, num_detections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J56pb8GqxeYQ"},"source":["#좌표값이 0~1사이로 정규화 되지 않고 원본 이미지의 좌표값으로 반환 \n","print('원본 이미지 shape:', img_array.shape)\n","boxes[0, 0:10], scores[0, :10], classes[0, :10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnGP-aVbigHg"},"source":["labels_to_names = {1:'person',2:'bicycle',3:'car',4:'motorcycle',5:'airplane',6:'bus',7:'train',8:'truck',9:'boat',10:'traffic light',\n","                    11:'fire hydrant',12:'street sign',13:'stop sign',14:'parking meter',15:'bench',16:'bird',17:'cat',18:'dog',19:'horse',20:'sheep',\n","                    21:'cow',22:'elephant',23:'bear',24:'zebra',25:'giraffe',26:'hat',27:'backpack',28:'umbrella',29:'shoe',30:'eye glasses',\n","                    31:'handbag',32:'tie',33:'suitcase',34:'frisbee',35:'skis',36:'snowboard',37:'sports ball',38:'kite',39:'baseball bat',40:'baseball glove',\n","                    41:'skateboard',42:'surfboard',43:'tennis racket',44:'bottle',45:'plate',46:'wine glass',47:'cup',48:'fork',49:'knife',50:'spoon',\n","                    51:'bowl',52:'banana',53:'apple',54:'sandwich',55:'orange',56:'broccoli',57:'carrot',58:'hot dog',59:'pizza',60:'donut',\n","                    61:'cake',62:'chair',63:'couch',64:'potted plant',65:'bed',66:'mirror',67:'dining table',68:'window',69:'desk',70:'toilet',\n","                    71:'door',72:'tv',73:'laptop',74:'mouse',75:'remote',76:'keyboard',77:'cell phone',78:'microwave',79:'oven',80:'toaster',\n","                    81:'sink',82:'refrigerator',83:'blender',84:'book',85:'clock',86:'vase',87:'scissors',88:'teddy bear',89:'hair drier',90:'toothbrush',\n","                    91:'hair brush'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWWkwWUGySi4"},"source":["def get_detected_img_automl(model, img_array, score_threshold, object_show_count=100, is_print=True):   \n","  # automl efficent은 반환 bbox 좌표값이 원본 이미지 좌표값으로 되어 있으므로 별도의 scaling작업 필요 없음. \n","  '''\n","  height = img_array.shape[0]\n","  width = img_array.shape[1]\n","  '''\n","  # cv2의 rectangle()은 인자로 들어온 이미지 배열에 직접 사각형을 업데이트 하므로 그림 표현을 위한 별도의 이미지 배열 생성. \n","  draw_img = img_array.copy()\n","\n","  # bounding box의 테두리와 caption 글자색 지정\n","  green_color=(0, 255, 0)\n","  red_color=(0, 0, 255)\n","\n","  # cv2로 만들어진 numpy image array를 tensor로 변환\n","  img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n","  #img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)[tf.newaxis, ...]\n","\n","  # efficientdet 모델을 다운로드 한 뒤 inference 수행. \n","  start_time = time.time()\n","  # automl efficientdet 모델은 boxes, score, classes, num_detections를 각각 Tensor로 반환. \n","  boxes, scores, classes, num_detections = model(img_tensor)\n","  # Tensor값을 시각화를 위해 numpy 로 변환. \n","  boxes = boxes.numpy()\n","  scores = scores.numpy()\n","  classes = classes.numpy()\n","  num_detections = num_detections.numpy()\n","  \n","  # detected 된 object들을 iteration 하면서 정보 추출. detect된 object의 갯수는 100개\n","  for i in range(num_detections[0]):\n","    # detection score를 iteration시 마다 높은 순으로 추출하고 SCORE_THRESHOLD보다 낮으면 loop 중단. \n","    score = scores[0, i]\n","    if score < score_threshold:\n","      break\n","    # detected된 object들은 scale된 기준으로 예측되었으므로 다시 원본 이미지 비율로 계산\n","    box = boxes[0, i]\n","\n","    ''' **** 주의 ******\n","    box는 ymin, xmin, ymax, xmax 순서로 되어 있음. 또한 원본 좌표값으로 되어 있음. '''\n","    left = box[1]\n","    top = box[0] \n","    right = box[3] \n","    bottom = box[2] \n","\n","    # class id 추출하고 class 명으로 매핑\n","    class_id = classes[0, i]\n","    caption = \"{}: {:.4f}\".format(labels_to_names[class_id], score)\n","    print(caption)\n","    #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n","    cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), color=green_color, thickness=2)\n","    cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, red_color, 1)\n","\n","  if is_print:\n","    print('Detection 수행시간:',round(time.time() - start_time, 2),\"초\")\n","\n","  return draw_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gcK2LLVwUGQ"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/baseball01.jpg'), cv2.COLOR_BGR2RGB)\n","draw_img = get_detected_img_automl(detector_automl_lite0, img_array, score_threshold=0.3, object_show_count=100, is_print=True)\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVRVHRWbwjyA"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/beatles01.jpg'), cv2.COLOR_BGR2RGB)\n","img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n","draw_img = get_detected_img_automl(detector_automl_lite0, img_array, score_threshold=0.3, object_show_count=100, is_print=True)\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGFs0dCDj46S"},"source":["### EfficientDet lite2 모델로 inference 수행."]},{"cell_type":"code","metadata":{"id":"faoa7o6sjN4h"},"source":["detector_automl_lite2 = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/lite2/detection/1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddvmzivUjrTS"},"source":["img_array = cv2.cvtColor(cv2.imread('/content/data/beatles01.jpg'), cv2.COLOR_BGR2RGB)\n","img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)[tf.newaxis, ...]\n","draw_img = get_detected_img_automl(detector_automl_lite2, img_array, score_threshold=0.5, object_show_count=100, is_print=True)\n","plt.figure(figsize=(12, 12))\n","plt.imshow(draw_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PC3XJzEU133m"},"source":["def do_detected_video_automl(model, input_path, output_path, score_threshold, is_print):\n","    \n","    cap = cv2.VideoCapture(input_path)\n","\n","    codec = cv2.VideoWriter_fourcc(*'XVID')\n","\n","    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size) \n","\n","    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    print('총 Frame 갯수:', frame_cnt)\n","\n","    green_color=(0, 255, 0)\n","    red_color=(0, 0, 255)\n","    while True:\n","        hasFrame, img_frame = cap.read()\n","        if not hasFrame:\n","            print('더 이상 처리할 frame이 없습니다.')\n","            break\n","        # get_detected_img_automl() 호출 \n","        img_frame = get_detected_img_automl(model, img_frame, score_threshold=score_threshold, object_show_count=100, is_print=is_print)\n","        \n","        vid_writer.write(img_frame)\n","    # end of while loop\n","\n","    vid_writer.release()\n","    cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkcRmgzo2OgM"},"source":["do_detected_video_automl(detector_automl_lite2, '/content/data/Jonh_Wick_small.mp4', './data/John_Wick_small_lite_02.mp4', 0.5, True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLQ6pML84kNF"},"source":[""],"execution_count":null,"outputs":[]}]}